{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayush-96/msc-data-science/blob/master/deep_learning/DL_lectureNotebook1_tensors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDpQ-i6p0Tey"
      },
      "source": [
        "#Getting started with tensors in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRRuF0X8xnmf"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWncMBGN1rA6"
      },
      "source": [
        "# Data representation in neural networks\n",
        "\n",
        "Modern machine-learning systems use *tensors* as their basic data structures. These are fundamentally containers for data. You will already have used matrices, which are examples of two-dimensional tensors. The tensor's *rank* is its number of axes (like dimensions for a matrix)\n",
        "\n",
        "# Tensors\n",
        "\n",
        "*  Scalars: Tensors which contain only one number. 0-dimensional tensors\n",
        "*  Vectors (1D tensors). An array of numbers is a vector or 1D tensor, and has one axis.\n",
        "*  Matrices are 2D tensors. The two axes are referred to as *rows* and *columns*\n",
        "*  3D tensors and higher. Putting a 2D matrix in a new array gives you a 3D tensor (and so on to higher dimensions..). You will end up spending a lot of time making sure your tensors are the right shape, when debugging your deep networks... :-)\n",
        "\n",
        "* Manipulating tensors in Numpy\n",
        "* data batches\n",
        "* Examples of training data used, in each case with $N=$ `samples` examples in the set:\n",
        "   * Vector data - 2D `(samples, features)`\n",
        "   * Time-series/sequence data - 3D tensors shaped `(samples, timesteps, features)`)\n",
        "   \n",
        "   ![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRrnL1abPz6Nkugh29zNvaq-L_KqAvJAIBJQdr_dpi4Khpnrn_sww)\n",
        "   * Image data -- 4D tensors shaped `(samples, height, width, channels)`\n",
        "   \n",
        "   ![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRbVhpi2ZiSb7fl0Q8YMXBhMQ7Ny-rwHG82TEpjgIm8yKCfiV0r)\n",
        "   * Video data - 5D tensors shaped `(samples, frames, height, width, channels)`. Think about the memory requirements of these tensors. How much memory would you need for a 60 second video at 256x256 sampled 30 times a second?\n",
        "---\n",
        "# Tensor operations\n",
        "\n",
        "* Element-wise operations - e.g. the activation function `relu()` when implemented as `torch.max(z,0.0)` is applied independently to each entry of the tensor considered (rather than using a `for` loop to run through each entry). Usually much more efficient than implementing loops. In a number of cases these element-wise operations are different from the typical operations on matrices. So element-wise multiplication corresponds to the Hadamard product $z = x \\odot y$ (sometimes written $x \\circ y$) where each element is multiplied together to get a new element rather than the typical matrix multiplication.\n",
        "* Broadcasting - when the shapes of tensors differs and there is no ambiguity, and the results are legal, the smaller tensor can be *broadcasted* to match the shape of the larger tensor.\n",
        "     * this has two steps:\n",
        "     1. axes are added to the smaller tensor to match the `ndim` of the larger (from the left-hand side)\n",
        "     2. the smaller tensor is repeated alongside any axes (assuming they are a multiple of the larger axes size) to match the full shape of the larger tensor.\n",
        "     E.g.\n",
        "     \n",
        "     `x = torch.rand(64,3,32,10)`\n",
        "     \n",
        "    `y = torch.rand(3, 32, 1)`\n",
        "    \n",
        "    `z = torch.max(x,y)` - z has shape (64,3,32,10) like `x`\n",
        "    \n",
        "* Tensor dot - aka *tensor product* . In torch use `z=torch.dot(x,y)`, in mathematical notation $z=x \\cdot y$. Note this is different from the elementwise multiplication `z=x*y`. The dot product of two vectors (which have to be the same size) is a scalar. The dot product of a matrix $x$ and a vector $y$ is a vector where the cofficients are the dot products between $y$ and the rows of $x$.\n",
        "* Tensor reshaping\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8NSa2l1aCGh",
        "outputId": "0a89cd3a-ff99-4280-fce1-e2bcc62eb137",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x=torch.ones(64, 3, 32, 10)\n",
        "y=torch.ones(3, 32, 1)\n",
        "z = torch.max(x, y)\n",
        "print(z)\n",
        "print(z.size())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          ...,\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n",
            "torch.Size([64, 3, 32, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us0Ekrytxnmp"
      },
      "source": [
        "Note that when an uninitialized matrix is created, whatever values were in the allocated memory at the time will appear as the initial values.\n",
        "\n",
        "An alternative is to explicitly set things to be random:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eQ31svwxnmq",
        "outputId": "bb55e3c1-1dd9-4948-9bf6-f196bdd613e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = torch.rand(5, 3)\n",
        "print(x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3286, 0.9708, 0.4427],\n",
            "        [0.0933, 0.3032, 0.5123],\n",
            "        [0.0994, 0.5312, 0.8439],\n",
            "        [0.0180, 0.0182, 0.1249],\n",
            "        [0.0238, 0.8175, 0.2946]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3HbYvlUxnmt"
      },
      "source": [
        "Construct a matrix filled zeros and of dtype long:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE59Jus1xnmu",
        "outputId": "7c8a1651-e04d-48aa-f689-a5f9feb9b8e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = torch.zeros(5, 3, dtype=torch.long)\n",
        "print(x)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-MIpgIJxnmx"
      },
      "source": [
        "Let us take a list of three numbers in python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qJSZM2Uxnmy"
      },
      "source": [
        "a = [1.0, 2.0, 1.0]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY0NJdpFxnm1"
      },
      "source": [
        "we can access the first element of the list using the index 0:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNCBoQNPxnm2",
        "outputId": "8feb69ca-db20-43ef-b822-d0ec293aff4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq2XMYJHxnm4",
        "outputId": "0c89d9e0-2238-40dc-b08b-ea32c7a043ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a[2] = 3.0\n",
        "a"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0, 2.0, 3.0]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NsruzDGxnm7"
      },
      "source": [
        "We can create a PyTorch tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCKbW_-qxnm8",
        "outputId": "3c15b4d4-0dfd-4c66-b1a8-1c6577953ed0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "a = torch.ones(3)\n",
        "a"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9-D-648xnm_"
      },
      "source": [
        "The first entry isn't a 1, it is a tensor with one element in it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZbU3QAyxnnA",
        "outputId": "cbe1f532-7b67-4dd9-f60b-84ac1c3c4c86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbARxFEDxnnC"
      },
      "source": [
        "but you can easily convert it to a float:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFoVVplYxnnD",
        "outputId": "ce688c3f-915f-44ba-9b0a-7bf752e7be43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "float(a[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRh9xUE51FpC",
        "outputId": "1ef1d652-c596-4395-976c-b097603e37e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a[0].detach().numpy()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(1., dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2RHmhlMxnnF",
        "outputId": "d42f0e98-5201-415d-b11a-2be70e5f699d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a[1] = 2.0\n",
        "a"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl-vnqT2xnnH"
      },
      "source": [
        "We can construct a tensor directly from data, by passing a Python list to the constructor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp0lEVd0xnnI",
        "outputId": "84f12ccf-8ad1-45e3-a5d0-6e5a5b7d279d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points = torch.tensor([5.5, 3, 6, 10])\n",
        "print(points)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 5.5000,  3.0000,  6.0000, 10.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOV1YuC7xnnK"
      },
      "source": [
        "This has the same result as the code below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EXlVZHfxnnL",
        "outputId": "63392cd0-ed50-4fd1-ada9-7f29e744ebde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points = torch.zeros(4) # <1>\n",
        "points[0] = 5.5 # <2>\n",
        "points[1] = 3.0\n",
        "points[2] = 6.0\n",
        "points[3] = 10.0\n",
        "print(points)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 5.5000,  3.0000,  6.0000, 10.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cVj548QxnnM"
      },
      "source": [
        "or create a tensor based on an existing tensor. These methods will reuse properties of the input tensor, e.g. dtype, unless new values are provided by user"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1NphnstxnnN",
        "outputId": "688b565c-58fe-4ed3-a4f4-5ab21c50c3dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes\n",
        "\n",
        "print(x)\n",
        "\n",
        "x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
        "\n",
        "print(x)                                      # result has the same size"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]], dtype=torch.float64)\n",
            "tensor([[ 1.1413,  1.2294,  0.3738],\n",
            "        [-1.0510,  0.0579,  0.3141],\n",
            "        [ 0.4817, -1.7826, -0.5173],\n",
            "        [-1.4999, -0.4972, -2.4130],\n",
            "        [ 0.2049, -1.1847, -0.8858]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjpdQILPxnnO",
        "outputId": "2dd54e68-6504-43da-8d86-77c63d44ccea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(x.size())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxnlG_CMxnnQ"
      },
      "source": [
        "Python lists or tuples of numbers are collections of Python objects that are individually allocated in memory. PyTorch tensors or NumPy arrays on the other hand are views over (typically) contiguous memory blocks containing  C numeric types unboxed rather than Python objects. For instance, float32 datatypes would each consist of 32-bit (4 byte) IEEE floating point values. This means that a 1D tensor of 1,000,000 float numbers will require exactly 4,000,000 contiguous bytes to be stored, plus a small overhead for the meta data (e.g. dimensions, numeric type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5apoNXwtxnnQ"
      },
      "source": [
        "What if we want to refer to 2D points? We can create a 2D tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGRZghDfxnnR",
        "outputId": "ccfad556-d6ea-4b83-d0ca-89ed10da1d80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "points"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 1.],\n",
              "        [5., 3.],\n",
              "        [2., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZk2XbklxnnT"
      },
      "source": [
        "and you can check the shape of a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GtP4LElxnnT",
        "outputId": "68eb67ab-262e-40d8-f879-c5692e7949ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc_YfvA3xnnX"
      },
      "source": [
        "You can also use zeros or ones to initialise the tensor, giving it a specific shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVow0LVQxnnY",
        "outputId": "fa0a02ac-5184-4289-aee8-39ca1307b10d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points = torch.zeros(3, 2)\n",
        "points"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgxVb5cGLVP_",
        "outputId": "67b334b5-8d0d-4f88-8dde-ae9c98c8ac74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points = torch.ones(3,2)\n",
        "points"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcHvm_SKLb38"
      },
      "source": [
        "By default, tensors are created using datatype torch.float32 (32-bit floating point numbers). However, different datatypes for the elements can be specified with the dtype argument, or there are also methods that create tensors with a specific datatype."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKqgeOxFQ3Q1",
        "outputId": "792cd9ac-b8b8-4a7f-ad4e-779a719f6b3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], dtype=torch.float32)\n",
        "print(points)\n",
        "print(points.dtype)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4., 1.],\n",
            "        [5., 3.],\n",
            "        [2., 1.]])\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cOioU7Exnna",
        "outputId": "88bafb71-8ae3-44f8-985b-581cb6bf3efc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ppoints = torch.FloatTensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "print(points)\n",
        "print(points.dtype)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4., 1.],\n",
            "        [5., 3.],\n",
            "        [2., 1.]])\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgbDsiuGxnnc"
      },
      "source": [
        "If we wanted the $y$ coordinate of the 0th point:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G93PmXk5xnnd",
        "outputId": "6a8c55e0-13f4-40dd-9a75-1c922ce568a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points[0, 1]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL9NIUbtxnne"
      },
      "source": [
        "or just get the full 2D coordinates of the 0th point:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0Wtv--Cxnnf",
        "outputId": "46b6f362-05c2-4d32-85c6-e184da1d9144",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yBPzCh_xnnh"
      },
      "source": [
        "***Views on storage***\n",
        " Values in Tensors are allocated in contiguous chunks of memory, managed by `torch.Storage` instances. A storage is a one-dimensional array of numerical data, i.e. a contiguous block of memory containing numbers of a given type, such as `float`, 32-bits representing a floating point number, or `int64`, 64-bits representing an integer. A PyTorch  `Tensor` is a view over such a  `Storage` that is capable of indexing into that storage using an offset and and per-dimension strides.\n",
        "\n",
        " Multiple tensors can index the same storage, even if they index into the data differently. When we requested  `points[0]` above, what we got back is another tensor that indexes the same storage as the  tensor, just not all of it and points with different dimensionality (1D vs 2D).  The underlying memory is allocated only once, however, so creating alternate tensor-views on the data can be done quickly, no matter the size of the data managed by the `Storage` instance.\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQznXczcxnnh"
      },
      "source": [
        "**Indexing into storage** Let’s see how indexing into the storage works in practice with our 2D points. The storage for a given tensor is accessible using the `.storage` property:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnAwQsqAxnni",
        "outputId": "16ec5115-e6ff-4ec7-eb10-2d7839f5be6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "points.storage()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-8575ff47c9e0>:2: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  points.storage()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 4.0\n",
              " 1.0\n",
              " 5.0\n",
              " 3.0\n",
              " 2.0\n",
              " 1.0\n",
              "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZfNmQM-xnnk"
      },
      "source": [
        "Even though the tensor reports itself as having 3 rows and 2 columns, the storage under the hood is a contiguous array of size 6. In this sense, the tensor just knows how to translate a pair of indices into a location in the storage. We can also index into a storage manually, for instance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ej234D9Txnnk",
        "outputId": "e4cfae06-5d9e-444c-98a7-9bcd00530b84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points_storage = points.storage()\n",
        "print(type(points_storage))\n",
        "print(points_storage[0])   # just a list - torch.Storage\n",
        "print(points.storage()[1])  # acutal tensor calling storage() to get values\n",
        "print(points.storage()[2])\n",
        "\n",
        "print(type(points.storage()[1]))  #float"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.storage.TypedStorage'>\n",
            "4.0\n",
            "1.0\n",
            "5.0\n",
            "<class 'float'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS9sT6kjxnnl"
      },
      "source": [
        "If you have a one element tensor, use .item() to get the value as a Python number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_SB7up0xnnm",
        "outputId": "18093f83-f5ee-4c78-9246-a2e13185a12a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(points[0,1].item()) #only works for 1 item tensor"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypnBQVYcxnno"
      },
      "source": [
        "We can’t index a storage of a 2D tensor using two indices. The layout of a storage is always one-dimensional, irrespective of the dimensionality of any and all tensors that might refer to it.\n",
        "\n",
        "Changing the value of a storage leads to changing the content of its referring tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsWEyL_3xnnp",
        "outputId": "21fe2859-aba3-4a18-dea0-7ff607767d04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "points_storage = points.storage()\n",
        "points_storage[0] = 2.0\n",
        "points"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2., 1.],\n",
              "        [5., 3.],\n",
              "        [2., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLsg-upZxnnq"
      },
      "source": [
        "In order to index into a storage, tensors rely on a few pieces of information, which, together with their storage, unequivocally define them: size, storage offset and stride.  The size (or shape, in NumPy parlance) is a tuple indicating how many elements across each dimension the tensor represents. The storage offset is the index in the storage corresponding to the first element in the tensor. Stride is the number of elements in the storage that need to be skipped over to obtain the next element along each dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9H5tEVkxnnr",
        "outputId": "06005bb6-f7d0-458b-c1bd-5a6ded00bf5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "second_point = points[1]\n",
        "print(second_point.storage_offset())\n",
        "\n",
        "second_point.size(), second_point.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2]), torch.Size([2]))"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS2mXI67xnns"
      },
      "source": [
        "The resulting tensor has offset 2 in the storage (since we need to skip the first point, which has two items) and the size is an instance of the `Size` class containing one element, since the tensor is one-dimensional. Important note: this is the same information as contained in the `shape` property of tensor objects:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTUCNbgtxnnt"
      },
      "source": [
        "Last, stride is a tuple indicating the number of elements in the storage that have to be skipped when the index is increased by 1 in each dimension. For instance, our `points`  tensor has a stride of : (2, 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ihnx4YBxnnt",
        "outputId": "2f10aeb6-7193-49aa-dff3-84419fd236d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points.stride()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5AV3s8Axnnv"
      },
      "source": [
        "This indirection between a tensor and its storage leads to some operations, like transposing a tensor or extracting a sub-tensor, to be inexpensive, as they do not lead to memory reallocations; instead they consist in allocating a new tensor object with a different value for size, storage offset or stride.\n",
        "\n",
        "Let’s try with transposing now. Let’s take our  tensor, that has individual points in the points rows and x and y coordinates in the columns, and turn it around so that individual points are along the columns. We take this opportunity to introduce the  function, a short-hand alternative t for 2-dimensional tensors\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLmwrN-0xnnv",
        "outputId": "89ed26e2-4967-40f8-f0bd-f3f1bc87783c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "points"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 1.],\n",
              "        [5., 3.],\n",
              "        [2., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO-A4W0Kxnnx",
        "outputId": "4a3bcd88-db13-44f3-fad6-530661ac5746",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points_t = points.t()\n",
        "points_t"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 5., 2.],\n",
              "        [1., 3., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSZIf7oTxnny"
      },
      "source": [
        "We can easily verify that the two tensors share the same storage:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD0NzKWfxnnz",
        "outputId": "abaf90be-7a50-4f9c-82f3-e87beb91623f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "id(points.storage()) == id(points_t.storage())\n",
        "points.storage()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 4.0\n",
              " 1.0\n",
              " 5.0\n",
              " 3.0\n",
              " 2.0\n",
              " 1.0\n",
              "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE5vh50Nxnn0"
      },
      "source": [
        "and that they differ only in the shape and stride\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaW9Fvy1xnn1",
        "outputId": "23dcb22a-864a-429c-8aa8-6c32fcb39add",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points.stride()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt3L7h4Nxnn2",
        "outputId": "51b44fd7-4d89-4bea-def2-ee0eb82cce01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points_t.stride()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77h284bwxnn4"
      },
      "source": [
        "Transposing in PyTorch is not limited to matrices. We can transpose a multidimensional array by specifying the two dimensions along which transposing (i.e. flipping shape and stride) should occur"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGhi_PYcxnn5",
        "outputId": "f309d433-1cb1-456e-9074-9d90190a7b4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "some_t = torch.ones(3, 4, 5)\n",
        "transpose_t = some_t.transpose(0, 2)\n",
        "some_t.shape"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YelLEqtzxnn7",
        "outputId": "c6eec355-c8c5-46ce-f4b3-5f3d1a92a173",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "transpose_t.shape"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 4, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p4kxKI1xnn8",
        "outputId": "aa93af3d-3a44-474f-e2fb-f276da3abc44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "some_t.stride()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 5, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRus_QGkxnn-",
        "outputId": "60efea3c-7c63-4185-fedf-6b29741ada12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "transpose_t.stride()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 5, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN6jYO6bxnoE",
        "outputId": "7c241f1c-0047-468f-a181-bfe2ceca7118",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points.is_contiguous()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyx6dUKKxnoF",
        "outputId": "cee97144-3d14-4cf9-bc05-30b4a7cf9037",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points_t.is_contiguous()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av3xEzW1xnoH",
        "outputId": "7ead3ca3-0e36-4b94-ab44-27e0bf5b30f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "points_t = points.t()\n",
        "points_t"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 5., 2.],\n",
              "        [1., 3., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfbWAIZKxnoI",
        "outputId": "db9d2513-d1e9-4977-eead-a00286433340",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points_t.storage()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 4.0\n",
              " 1.0\n",
              " 5.0\n",
              " 3.0\n",
              " 2.0\n",
              " 1.0\n",
              "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0ZBtfIhxnoK",
        "outputId": "6c1d2287-a76b-443b-e618-27cac9368ace",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points_t.stride()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF0DEt_nqlYY"
      },
      "source": [
        "The contiguous() method returns a tensor which has had memory reallocated so that the data elements are contiguous in memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnfiUZnMxnoN",
        "outputId": "10b7efba-7ec5-4952-aa27-88d791960e28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points_t_cont = points_t.contiguous()\n",
        "points_t_cont"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 5., 2.],\n",
              "        [1., 3., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbWx-l92xnoO",
        "outputId": "d2e9ca6d-4157-4b74-a003-776bc567a5fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points_t_cont.stride()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6WFYUPDxnoQ",
        "outputId": "84f318e1-0fb1-498b-9c33-c91bf11157d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points_t_cont.storage()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 4.0\n",
              " 5.0\n",
              " 2.0\n",
              " 1.0\n",
              " 3.0\n",
              " 1.0\n",
              "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RDto5UcrZkW"
      },
      "source": [
        "Tensors of different datatypes can be allocated by specifying the `dtype` attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evoMGCtaxnoS"
      },
      "source": [
        "double_points = torch.ones(10, 2, dtype=torch.double)\n",
        "short_points = torch.tensor([[1, 2], [3, 4]], dtype=torch.short)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJCnDOIXxnoT",
        "outputId": "c795be49-2f2b-41e5-8ce7-1c59244772ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(double_points.dtype)\n",
        "print(short_points.dtype)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float64\n",
            "torch.int16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AStk5MKWr4OY"
      },
      "source": [
        "Alternatively, methods such as `double()` can be used to convert the datatype of a current tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sIPa7sSxnoV",
        "outputId": "b4be0f3a-ba81-4f17-dbee-4387ba3eff7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "double_points = torch.zeros(5, 2).double()\n",
        "short_points = torch.ones(5, 2).short()\n",
        "print(double_points)\n",
        "print(short_points)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.]], dtype=torch.float64)\n",
            "tensor([[1, 1],\n",
            "        [1, 1],\n",
            "        [1, 1],\n",
            "        [1, 1],\n",
            "        [1, 1]], dtype=torch.int16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQUjGTissLll"
      },
      "source": [
        "The `to()` method can also be used to convert tensors to a particular torch datatype."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cszxvax-xnoX",
        "outputId": "bcde1533-b2b6-4637-da39-09307c64046c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "double_points = torch.zeros(5, 2).to(torch.double)\n",
        "short_points = torch.ones(5, 2).to(dtype=torch.short)\n",
        "print(double_points)\n",
        "print(short_points)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.]], dtype=torch.float64)\n",
            "tensor([[1, 1],\n",
            "        [1, 1],\n",
            "        [1, 1],\n",
            "        [1, 1],\n",
            "        [1, 1]], dtype=torch.int16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ4eN3Qqs135"
      },
      "source": [
        "And finally the method `type()` can also be used to convert tensors to a particular type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USWZzSKTxnoY",
        "outputId": "a31a8fd3-e691-4972-c7bd-6a3d750d4891",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "points = torch.randn(5, 2)\n",
        "print(points)\n",
        "short_points = points.type(torch.short)\n",
        "print(short_points)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.8530, -1.3632],\n",
            "        [-0.5059, -0.3222],\n",
            "        [-0.9537,  0.2737],\n",
            "        [-2.6206,  1.5999],\n",
            "        [-1.1712,  0.8567]])\n",
            "tensor([[ 0, -1],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [-2,  1],\n",
            "        [-1,  0]], dtype=torch.int16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjhjIMUGxnoa"
      },
      "source": [
        "# reset points back to original value\n",
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3_jV140xnob"
      },
      "source": [
        "***Indexing Tensors***\n",
        "What if we need to obtain a tensor containing all points but the first? That’s easy using range indexing notation, the same that applies to standard Python lists, which we quickly recall. (Add print statements to the other entries if you are unsure what they will produce as output.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G93Yu9OJxnob",
        "outputId": "a94c9a90-f0bf-43c4-e469-f75f7cc10fd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "some_list = list(range(6))\n",
        "print(some_list)\n",
        "some_list[:]            # <1>\n",
        "some_list[1:4]          # <2>\n",
        "some_list[1:]           # <3>\n",
        "some_list[:4]           # <4>\n",
        "some_list[:-1]          # <5>\n",
        "print(some_list[1:4:2]) # <6>"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5]\n",
            "[1, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEqs-Z-9xnoc"
      },
      "source": [
        "1. all elements in the list\n",
        "2. from element 1 inclusive to element 4 exclusive\n",
        "3. from element 1 inclusive to the end of the list\n",
        "4. from the start of the list to element 4 exclusive\n",
        "5. from the start of the list to one before the last element\n",
        "6. from element 1 inclusive to element 4 exclusive in steps of 2\n",
        "\n",
        "To achieve our goal we can use the same notation for PyTorch tensors, with the added benefit that, just like in NumPy and in other Python scientific libraries, we can use range indexing for each of the dimensions of the tensor:\n",
        "\n",
        "(Again add print statements if you are uncertain what values are produced by any of these statements.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ1OTMx3xnod",
        "outputId": "81bdfb6a-a134-43c6-bf72-83e2f4ee05e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(points)\n",
        "print(points[1:])          # <1>\n",
        "print(points[1:, :])       # <2>\n",
        "print(points[1:, 0])       # <3>\n",
        "print(points[None]) # <4>"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4., 1.],\n",
            "        [5., 3.],\n",
            "        [2., 1.]])\n",
            "tensor([[5., 3.],\n",
            "        [2., 1.]])\n",
            "tensor([[5., 3.],\n",
            "        [2., 1.]])\n",
            "tensor([5., 2.])\n",
            "tensor([[[4., 1.],\n",
            "         [5., 3.],\n",
            "         [2., 1.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAsHejolxnoe"
      },
      "source": [
        "1. All rows after first, implicitly all columns\n",
        "2. All rows after first, all columns\n",
        "3. All rows after first, first column\n",
        "4. Add dimension of size one, just like unsqueeze (note the extra square brackets printed compared to just points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-b_u-Jpxnoe"
      },
      "source": [
        "***Named Tensors***\n",
        "The dimensions (or axes) of our Tensors usually index something like pixel locations or color channels. This means that when we want to index into our Tensor, we need to remember the ordering of the dimensions and write our indexing accordingly. As data is transformed through multiple tensors, keeping track of which dimension contains what data can be error-prone.\n",
        "To make things concrete, imagine that we have a 3D Tensor like `img_t` (we will use dummy data for simplicity here) and want to convert it to grayscale. We looked up typical weights for the colors to derive a single brightness value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuj59ECQxnoh"
      },
      "source": [
        "img_t = torch.randn(3, 5, 5) # shape [channels, rows, columns]\n",
        "weights = torch.tensor([0.2126, 0.7152, 0.0722])\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJa3p4lzxnoj"
      },
      "source": [
        "We also often want our code to generalize - for example from grayscale images represented as 2D Tensors with height and width dimensions to color images adding a third channel dimension (as in RGB) or from a single image to a batch of images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hb7EqYQxnok"
      },
      "source": [
        "batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns]"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCut_4NFxnom"
      },
      "source": [
        "So sometimes the RGB channels are in dimension 0 and sometimes in dimension 1. But we can generalize by counting from the end: They are always in dimension -3, the third from the end. The lazy, unweighted mean would thus be written as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilAe0YwExnom",
        "outputId": "070dba43-a79e-4cda-e432-a4cfc9bcaeb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "img_gray_naive = img_t.mean(-3)\n",
        "batch_gray_naive = batch_t.mean(-3)\n",
        "\n",
        "img_gray_naive.shape, batch_gray_naive.shape"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFJOw4iFxnoo"
      },
      "source": [
        "But now we have the weight, too. PyTorch will allow us to multiply things that are of same shape, but also of shapes where one operand is of size one in a given dimension. It also appends leading dimensions of size one automatically. This is a feature called *broadcasting*. We see that our `batch_t` of shape (2, 3, 5, 5) gets multiplied with the `unsqueezed_weights` of shape (3, 1, 1) to a tensor of shape (2, 3, 5, 5), from which we can then sum the third dimension from the end (the 3 channels).\n",
        "\n",
        "Make sure you understand how the `unsqueeze()` method is working from the printed outputs (count the square brackets on the output!), and then also how the broadcasting is working when doing multiplications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e10LpIAIxnop",
        "outputId": "544f7148-bd81-47a7-e6ea-b1cfaac76447",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f\"Shape of weights: {weights.shape}\")\n",
        "print(f\"Value: {weights} \\n\")\n",
        "\n",
        "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze(-1)\n",
        "print(f\"Shape of unsqueezed weights: {unsqueezed_weights.shape}\")\n",
        "print(f\"Value: {unsqueezed_weights}\\n\")\n",
        "\n",
        "img_weights = (img_t * unsqueezed_weights)\n",
        "batch_weights = (batch_t * unsqueezed_weights)\n",
        "img_gray_weighted = img_weights.sum(-3)\n",
        "batch_gray_weighted = batch_weights.sum(-3)\n",
        "\n",
        "print(f\"Shape of batch_gray_weighted: {batch_gray_weighted.shape}\")\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of weights: torch.Size([3])\n",
            "Value: tensor([0.2126, 0.7152, 0.0722]) \n",
            "\n",
            "Shape of unsqueezed weights: torch.Size([3, 1, 1])\n",
            "Value: tensor([[[0.2126]],\n",
            "\n",
            "        [[0.7152]],\n",
            "\n",
            "        [[0.0722]]])\n",
            "\n",
            "Shape of batch_gray_weighted: torch.Size([2, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV6f4JN1xnoq"
      },
      "source": [
        "Because this gets messy quickly (and for efficiency), there even is a PyTorch function einsum (adapted from NumPy) that specifies an indexing mini-language  giving index names to 28 dimensions for sums of such products. As often in Python, broadcasting — a form of summarizing unnamed things — is done using three dots `...`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzMJKaBmxnor",
        "outputId": "86b165aa-7d4a-45ca-a2d7-64e848d06514",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "img_gray_weighted_fancy   = torch.einsum('...chw,c->...hw', img_t, weights)\n",
        "batch_gray_weighted_fancy = torch.einsum('...chw,c->...hw', batch_t, weights)\n",
        "\n",
        "batch_gray_weighted_fancy.shape"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 5, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaDYvZZjxnou"
      },
      "source": [
        "PyTorch 1.3 added  as an experimental feature. Tensor factory functions such as `tensor`  or `rand` take a `names` argument. The names should be a sequence of strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3tgMq_dxnou",
        "outputId": "e05e05d4-87d5-4e0f-f07b-6bf384e052d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels'])\n",
        "\n",
        "weights_named\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-73-869f7b5687c0>:1: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1928.)\n",
            "  weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoJQmR7Jxnow"
      },
      "source": [
        "When we already have a tensor and want to add names (but not change existing ones), we can call the `refine_names` method  on it. Similar to indexing, the ellipsis `...` allows you to leave out refine_names … any number of dimensions. With the  `rename` sibling method you can also overwrite or drop (by passing in `None`) existing names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kunAYEsnxnox",
        "outputId": "d60dd386-d9d0-4efb-c228-c2b0e8fecb68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "img_named =  img_t.refine_names(..., 'channels', 'rows', 'columns')\n",
        "batch_named = batch_t.refine_names(..., 'channels', 'rows', 'columns')\n",
        "\n",
        "print(\"img named:\", img_named.shape, img_named.names)\n",
        "print(\"batch named:\", batch_named.shape, batch_named.names)\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img named: torch.Size([3, 5, 5]) ('channels', 'rows', 'columns')\n",
            "batch named: torch.Size([2, 3, 5, 5]) (None, 'channels', 'rows', 'columns')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYQhphHyxnoy"
      },
      "source": [
        "For operations with two inputs, in addition to the usual dimension checks, i.e. that sizes are either the same or one is 1 and can be broadcast to the other, PyTorch will now check the names for us. So far, it does not automatically align dimensions, so we need to do this explicitly. The method  `align_as` returns a tensor with missing dimensions added and existing ones permuted to the right order"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwQCsnCFxnoy",
        "outputId": "cadafa8d-1c83-4f33-8d64-17b346faa3bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "weights_aligned = weights_named.align_as(img_named)\n",
        "weights_aligned.shape, weights_aligned.names"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 1, 1]), ('channels', 'rows', 'columns'))"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T10CfSK7xnoz"
      },
      "source": [
        "Functions accepting dimension arguments, like `sum`, also take named dimensions. A nice feature for robustness is that if you try to combine dimensions with different names, you get an error.  Named tensors have the potential of eliminating many sources of alignment errors which are a frequent source of debugging problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-xwtYZuxnoz",
        "outputId": "93acff47-58fa-4c98-ff7f-2ad364904173",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "gray_named = (img_named * weights_aligned).sum('channels')\n",
        "\n",
        "gray_named.shape, gray_named.names"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5, 5]), ('rows', 'columns'))"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25l5sirYx-zN"
      },
      "source": [
        "The code below should create an error..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUrGFk3txno1",
        "outputId": "baebc600-7a25-45eb-95fa-b91d2b2e6660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "gray_named = (img_named[..., :3] * weights_named).sum('channels')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-4959ba21081c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgray_named\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_named\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights_named\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'channels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsY17nfNxno3"
      },
      "source": [
        "***Tensor Element Types***\n",
        "Python numeric types can be sub-optimal for several reasons:\n",
        "* Numbers in Python are full-fledged objects. while a floating point number might only take, for instance, 32 bits to be represented on a computer, Python will convert them in a full-fledged Python object with reference counting, etc.. This operation, called boxing, is not a problem if we need to store a small number of them, but allocating millions of such numbers gets very inefficient;\n",
        "* Lists in Python are meant for sequential collections of objects. there are no operations defined for, say, efficiently taking the dot product of two vectors, or summing vectors together; also, Python lists have no way of optimizing the layout of their content in memory, as they are indexable collections of pointers to Python objects (of any kind, not just numbers); last, Python lists are one-dimensional, and while one can create lists of lists, this is again very inefficient;\n",
        "* The Python interpreter is slow compared to optimized, compiled code. Performing mathematical operations on large collections of numerical data can be much faster using optimized code written in a compiled, low-level language like C.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRXiM_n8xno4"
      },
      "source": [
        "For these reasons, data science libraries rely on NumPy, or introduce dedicated data structures like PyTorch tensors, that provide efficient low-level implementations of numerical data structures and related operations on them, wrapped in a convenient high-level API. To enable this, the objects within a tensor must be all numbers of the same type and PyTorch must keep track of this numeric type.\n",
        "\n",
        "The `dtype` argument to tensor constructors (that is, functions like `tensor`, `zeros`, `ones`) specifies the numerical data (d) type that will be contained in the tensor. The data type specifies the possible values the tensor can hold (integers vs. floating point numbers) and the number of bytes per value.  The `dtype` argument is deliberately similar to the standard NumPy argument of the same name.\n",
        "\n",
        "Computations happening in neural networks are typically executed in 32-bit floating point precision. Higher precision, like 64-bit, will not buy us improvements in the accuracy of a model and will require more memory and computing time. The 16-bit floating point, half precision data type is not present natively in standard CPUs, but it is offered on modern GPUs. It is possible to switch to half-precision to decrease the footprint of a neural network model if needed, with minor impact on accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGZsoE6ixno5"
      },
      "source": [
        "PyTorch tensors can be converted to NumPy arrays and vice versa very efficiently. By doing so, we can leverage the huge swath of functionality in the wider Python ecosystem that has built up around the NumPy array type. This zero-copy interoperability with NumPy arrays is due to the storage system working with the Python buffer protocol Converting tensors to `numpy` arrays is very straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEMha9-Hxno5"
      },
      "source": [
        "points = torch.ones(3, 4)\n",
        "points_np = points.numpy()\n",
        "points_np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cg-qvoNxno6"
      },
      "source": [
        "which will return a NumPy multidimensional array of the right size, shape and numerical type. Interestingly, the returned array shares the same underlying buffer with the tensor storage. This means that the `numpy` method can be effectively executed at basically no cost, as long as the data  sits in CPU RAM. It also means that modifying the NumPy array will lead to a change in the originating tensor.\n",
        "If the tensor is allocated on the GPU, PyTorch will make a copy of the content of the tensor into a NumPy array allocated on the CPU.\n",
        "Vice-versa, we can obtain a PyTorch tensor from a NumPy array this way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DJDbWH8xno7"
      },
      "source": [
        "points = torch.from_numpy(points_np)\n",
        "points"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skkE5HKfxno8"
      },
      "source": [
        "While the default numeric type in PyTorch is 32 bit floating point, for the one for numpy it is 64 bit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymCxDxLhxno8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "a = torch.ones(5)\n",
        "print(a, a.dtype)\n",
        "b = a.numpy()\n",
        "print(b, b.dtype)\n",
        "\n",
        "c = np.ones(5)\n",
        "print(c, c.dtype)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn3ZFASKxno9"
      },
      "source": [
        "Note how the `numpy` array changes value if we change the `tensor`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqGJ69GRxno9"
      },
      "source": [
        "print(a)\n",
        "print(b)\n",
        "a.add_(1)\n",
        "print(a)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyc45I7mxnpB"
      },
      "source": [
        "***Serialising tensors***\n",
        "\n",
        "Creating a tensor on the fly is all well and fine, but if the data inside is of any value to us, we will want to save it to a file and load it back at some point. After all, we don’t want to have to retrain a model from scratch every time we start running our program! PyTorch uses `pickle` under the hood to serialize the tensor object, plus dedicated serialization code for the storage. Here’s how we can save our `points` tensor to an `ourpoints.t` file: (note that this won't work as is in `colab` but would be fine if you ran the notebook locally on your machine -- we'll cover colab file issues later)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow1F91yRxnpB"
      },
      "source": [
        "torch.save(points, './drive/../data/p1ch3/ourpoints.t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b99E4BkSYC5T"
      },
      "source": [
        "One way to save and load data from and to colab is to mount a google drive (you will get some space with your account if you created it for colab)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjmqXdoBX30f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFk5xlNcxnpC"
      },
      "source": [
        "As an alternative, we can pass a file descriptor in lieu of the filename"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-4a6L_OYXeo"
      },
      "source": [
        "torch.save(points, '/content/drive/MyDrive/ourpoints.t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHG5EnYqxnpC"
      },
      "source": [
        "with open('/content/drive/MyDrive/ourpoints.t','wb') as f:\n",
        "   torch.save(points, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWpKDg6uxnpD"
      },
      "source": [
        "points = torch.load('/content/drive/MyDrive/ourpoints.t')\n",
        "points"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaXNHWhuxnpF"
      },
      "source": [
        "with open('/content/drive/MyDrive/ourpoints.t','rb') as f:\n",
        "   points = torch.load(f)\n",
        "\n",
        "points"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ocH-FqoxnpI"
      },
      "source": [
        "While this is a way we can quickly save tensors in case we only want to load them with PyTorch, the file format itself is not interoperable. We can’t read the tensor with software other than PyTorch. Depending on the use case, this may or may not be a limitation, but we should learn how to save tensors interoperably for those times it is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKfiLH5WxnpI"
      },
      "source": [
        "***Serialising to HDF5 with h5py***\n",
        "\n",
        "For those cases when you need to, however, you can use the HDF5 format and library. HDF5 is a portable and widely supported format for representing serialized multidimensional arrays, organized in a nested key-value dictionary. Python supports HDF5 through the  library `h5py`, which accepts and returns data under the form of NumPy arrays.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIonJBmBxnpJ"
      },
      "source": [
        "import h5py\n",
        "\n",
        "f = h5py.File('/content/drive/MyDrive/ourpoints.hdf5', 'w')\n",
        "dset = f.create_dataset('coords', data=points.numpy())\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOBe_T1oxnpJ"
      },
      "source": [
        "Here `coords` is a key into the HDF5 file. We can have other keys, even nested ones. One of the interesting things in HDF5 is that we can index the dataset while on disk and access only the elements we are interested in. Let us suppose we want to load just the last two points in our dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjpHSQYlxnpK"
      },
      "source": [
        "f = h5py.File('/content/drive/MyDrive/ourpoints.hdf5', 'r')\n",
        "dset = f['coords']\n",
        "last_points = dset[-2:]\n",
        "last_points"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew8KQl8rxnpL"
      },
      "source": [
        "What happened here is that data has not been loaded when the file was opened or the dataset was required. Rather, data stayed on disk until we requested the second and last rows in the dataset. At that point `h5py`,  has accessed those two columns and returned a NumPy array-like object encapsulating that region in that dataset that behaves like a NumPy array and has the same API.\n",
        "Owing to this fact, we can pass the returned object to the `torch.from_numpy` function to obtain  a tensor directly. Note that in this case the data is copied over to the tensor’s storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_5gVEZHxnpL"
      },
      "source": [
        "last_points = torch.from_numpy(dset[-2:])\n",
        "f.close()\n",
        "last_points"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRiuldkaxnpP"
      },
      "source": [
        "***Moving tensors to the GPU***\n",
        " Every Torch tensor can be transferred to (one of) the GPU(s) in order to perform massively parallel, fast computations. All operations that will be performed on the tensor will be carried out using GPU-specific routines that come with PyTorch. In addition to the `dtype`, a PyTorch `Tensor` also has a notion of `device`, which is where on the computer the tensor data is being placed. Here is how we can create a tensor on the GPU by specifying the corresponding argument to the constructor:\n",
        "(Note: if running this in Colab you must have changed 'runtime' to 'GPU' for this to work - in the Edit/Notebook Settings menu item.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqoZM8f4xnpQ"
      },
      "source": [
        "import torch # Doing this again since your Notebook would have been reset if you changed settings.\n",
        "\n",
        "points_gpu = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device='cuda')\n",
        "points_gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3_8pRs7xnpR"
      },
      "source": [
        "We could instead copy a tensor created on the CPU onto the GPU using the  method `to()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6UFWdcYxnpR"
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device='cpu')\n",
        "points_gpu = points.to(device='cuda')\n",
        "\n",
        "points_gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EOQV9eYxnpT"
      },
      "source": [
        "Doing so returns a new tensor that has the same numerical data, but stored in the RAM of the GPU, rather than in regular system RAM. Now that the data is stored locally on the GPU, we’ll start to see the speedups mentioned earlier when performing mathematical operations on the tensor. In almost all cases, CPU- and GPU-based tensors expose the same user-facing API, making it much easier to write code that is agnostic to where, exactly, the heavy number crunching is running.\n",
        "\n",
        "In case our machine has more than one GPU, we can also decide on which GPU we allocate the tensor by passing a zero-based integer identifying the GPU on the machine, such as\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r50REDPlxnpU"
      },
      "source": [
        "points_gpu = points.to(device='cuda:0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQq-KkCGxnpV"
      },
      "source": [
        "points = 2 * points                         # <1>\n",
        "points_gpu = 2 * points.to(device='cuda')   # <2>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR6SjS9ExnpV"
      },
      "source": [
        "1. Multiplication performed on the CPU.\n",
        "2. Multiplication performed on the GPU.\n",
        "\n",
        "Note that the  tensor is not brought back to the CPU once the result has been points_gpu computed. What happened in the line above is that\n",
        "\n",
        "1) the  tensor has been copied to the GPU;\n",
        "2) a new tensor has been allocated on the GPU points and used to store the result of the multiplication;\n",
        "3) a handle to that GPU tensor is returned.\n",
        "\n",
        "Therefore, if we also add a constant to the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FOJWiGixnpW"
      },
      "source": [
        "points_gpu = points_gpu + 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8hV1XA-xnpX"
      },
      "source": [
        "the addition is still performed on the GPU, no information flows to the CPU (except if we print or access the resulting tensor). In order to move the tensor back to the CPU we need to provide a `cpu` argument to the `to` method, such as"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-Prn7LkxnpX"
      },
      "source": [
        "points_cpu = points_gpu.to(device='cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCq26o3-Ltz5"
      },
      "source": [
        "There are also specific methods to transfer tensors between the CPU and GPU(s)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnuxJC2axnpY"
      },
      "source": [
        "points_gpu = points.cuda()\n",
        "points_gpu = points.cuda(0)\n",
        "points_cpu = points_gpu.cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FjlpBUexnpZ"
      },
      "source": [
        "***The tensor API***\n",
        "\n",
        "Let's get a feel for the tensor operations that PyTorch offers, to give a feel for the API. The vast majority of operations on and between tensors are available under the `torch` module https://pytorch.org/docs/stable/torch.html and can also be called as methods of a tensor object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5-klzrwxnpZ"
      },
      "source": [
        "a = torch.ones(3, 2)\n",
        "a_t = torch.transpose(a, 0, 1)\n",
        "\n",
        "a.shape, a_t.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5FD1r__xnpa"
      },
      "source": [
        "or, for exactly the same result, as a method of the `a` tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rssecS1Kxnpa"
      },
      "source": [
        "a = torch.ones(3, 2)\n",
        "a_t = a.transpose(0, 1)\n",
        "\n",
        "a.shape, a_t.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7TSseWDxnpb"
      },
      "source": [
        "For more details look at the online docs http://pytorch.org/docs They are exhaustive and well organized, with the tensor operations divided into groups:\n",
        "* Creation ops — functions for constructing a tensor, like `ones`  and `from_numpy`\n",
        "* Indexing, slicing, joining, mutating ops — functions for changing the shape, stride or content a tensor, like `transpose`\n",
        "* Math ops — functions for manipulating the content of the tensor through computations\n",
        "    * Pointwise ops — functions for obtaining a new tensor by applying a function to each element independently, like `abs` and  `cos`\n",
        "    * Reduction ops — functions for computing aggregate values by iterating through tensors, like `mean`, `std` and `norm`\n",
        "    * Comparison ops — functions for evaluating numerical predicates over tensors, like `equal` and `max`\n",
        "    * Spectral ops — functions for transforming in and operating in the frequency domain, like `stft` and `hamming_window`\n",
        "    * Other operations — special functions operating on vectors, like `cross`, or matrices, like `trace`\n",
        "    * BLAS and LAPACK operations — functions following the BLAS (Basic Linear Algebra Subprograms) specification for scalar, vector-vector, matrix-vector and matrix-matrix operations\n",
        "* Random sampling — functions for generating values by drawing randomly from probability distributions, like `randn` and `normal`\n",
        "* Serialization — functions for saving and loading tensors, like `load` and `save`\n",
        "* Parallelism — functions for controlling the number of threads for parallel CPU execution, like `set_num_threads`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCDrPfaJxnpd"
      },
      "source": [
        "***Summary***\n",
        "\n",
        "* Neural networks transform floating point representations into other floating point representations, with the starting and ending representations typically being human-interpretable. The intermediate representations are less so.\n",
        "* These floating point representations are stored in Tensors.\n",
        "* Tensors are multidimensional arrays; they are the basic data structure in PyTorch.\n",
        "* PyTorch has a comprehensive standard library for tensor creation, manipulation and mathematical operations.\n",
        "* Tensors can be serialized to disk and loaded back.\n",
        "* All tensor operations in PyTorch can execute on the CPU as well as on the GPU, with no change in the code.\n",
        "* PyTorch uses a trailing underscore to indicate that a function operates in-place on a tensor (e.g. `Tensor.sqrt_`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCSKmTNvXy_z"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}