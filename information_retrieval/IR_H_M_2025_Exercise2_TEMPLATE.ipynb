{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayush-96/msc-data-science/blob/master/information_retrieval/IR_H_M_2025_Exercise2_TEMPLATE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spUoQOmD5xzt"
      },
      "source": [
        "# Information Retrieval Exercise 2 Notebook\n",
        "\n",
        "\n",
        "This is the template notebook for Exercise 2. The specification for the exercise and the corresponding Exercise 2 Quiz submission instance are available on the Moodle page of the course.\n",
        "\n",
        "This exercise builds upon Exercise 1, and assumes that you are now familiar with concepts we have introduced in both the Lab 1 and Exercise 1, including:\n",
        " - [PyTerrier operators](https://pyterrier.readthedocs.io/en/latest/operators.html)\n",
        " - [Pyterrier apply transformers](https://pyterrier.readthedocs.io/en/latest/transformer.html)\n",
        " - [PyTerrier pt.Experiment()](https://pyterrier.readthedocs.io/en/latest/experiments.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huHo3UfD5z-A"
      },
      "source": [
        "## PyTerrier Setup\n",
        "\n",
        "First, let's install PyTerrier as usual. We require a specific version of LightGBM. Do not change this version - if you are running locally on Apple Silicon, this wont work, and you should move back to Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1KQ6pjZ5dRb"
      },
      "source": [
        "%pip install -q python-terrier lightgbm==2.2.3 pyterrier-caching"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDst0SPaLY0U"
      },
      "source": [
        "Let's start PyTerrier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-6kc18m5dRk"
      },
      "source": [
        "import pyterrier as pt\n",
        "\n",
        "# we require a specific version of LightGBM for this exercise\n",
        "import lightgbm\n",
        "assert lightgbm.__version__ == '2.2.3'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to speed things up for you by caching the PL2 results and the standard feature set.\n",
        "\n",
        "DO NOT be tempted to cache your own feature implementations."
      ],
      "metadata": {
        "id": "emwrgiCio4bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyterrier_caching import RetrieverCache, SparseScorerCache\n",
        "\n",
        "CACHE=True"
      ],
      "metadata": {
        "id": "r8fx1vI3n8bN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3BwH1hz5dRl"
      },
      "source": [
        "## Index, Topics & Qrels for Exercise 2\n",
        "\n",
        "You will need your login & password credentials from Exercise 1. We will be using again the \"50pct\" and the \"trec-wt-2004\" datasets from Exercise 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd5zaOrL5dRm"
      },
      "source": [
        "UNAME=\"TODO\"\n",
        "PWORD=\"TODO\"\n",
        "\n",
        "# we will again be using the \"50pct\" and \"trec-wt-2004\" datasets\n",
        "Fiftypct = pt.get_dataset(\"50pct\",  user=UNAME, password=PWORD)\n",
        "dotgov_topicsqrels = pt.get_dataset(\"trec-wt-2004\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAW4E_7m7na0"
      },
      "source": [
        "On the other hand, you will be using a slightly updated index for Exercise 2. It is a bit bigger than the Exercise 1 index, hence it takes about 2-3 minutes to download to Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XYmUmh25dRl"
      },
      "source": [
        "\n",
        "indexref = Fiftypct.get_index(variant=\"ex3\")\n",
        "index = pt.IndexFactory.of(indexref, memory=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSWbTO8d5dRl"
      },
      "source": [
        "Let's check out the new index. Compared to the index we used for Exercise 1, you can see that this index has `Field Names: [TITLE, ELSE]`, which means that we can provide statistics about how many times each term occurs in the title of each document (the \"TITLE\" field), vs the rest of the document (the \"ELSE\" field). Refer to Lecture 7 for more information about fields.\n",
        "\n",
        "Let's also display the keys in the meta index - this is the metadata that we have stored for each document. You can see that we are storing the \"url\" and the \"body\" (content) of the document. These will particularly come in handy for Q2 and Q3 of Exercise 2, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp6-MDzY5dRl"
      },
      "source": [
        "print(index.getCollectionStatistics())\n",
        "print(\"In the meta index: \" + str(index.getMetaIndex().getKeys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdMiyeZ177AK"
      },
      "source": [
        "Finally, these are all of the topics and qrels (including the training and validation datasets) that you will need to conduct Exercise 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKi5ACEj5dRm"
      },
      "source": [
        "tr_topics = Fiftypct.get_topics(\"training\")\n",
        "va_topics = Fiftypct.get_topics(\"validation\")\n",
        "\n",
        "tr_qrels = Fiftypct.get_qrels(\"training\")\n",
        "va_qrels = Fiftypct.get_qrels(\"validation\")\n",
        "\n",
        "test_topics = dotgov_topicsqrels.get_topics(\"hp\")\n",
        "test_qrels = dotgov_topicsqrels.get_qrels(\"hp\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hWAMSAK5dRm"
      },
      "source": [
        "## Baseline Setup\n",
        "\n",
        "We introduce here the terrier.Retriever for our baseline. Note that:\n",
        " - We are using PL2 as our weighting model to generate the candidate set of documents to re-rank.\n",
        " - We expose more document metadata, namely \"url\" and \"body\" for each document retrieved, which you will need to deploy your two new features.\n",
        " - By setting `verbose=True`, we display a progress bar while retrieval executes.\n",
        " - We cache PL2 to make it faster for reuse in later experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3atqw8q35dRn"
      },
      "source": [
        "firstpass = pt.terrier.Retriever(index, wmodel=\"PL2\", metadata=[\"docno\", \"url\", \"body\"], verbose=True)\n",
        "if CACHE: # wrap in a cache transformer\n",
        "    firstpass = RetrieverCache('pl2-cache', firstpass)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iez_aX9D1pXI"
      },
      "source": [
        "Let's see the resulting output - you can see that there are now \"url\" and \"body\" attributes for each retrieved document. (We also display a progress bar, enabled by the `verbose=True`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkE-UBRd5dRn"
      },
      "source": [
        "firstpass.search(\"chemical reactions\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVQGefXx5dRo"
      },
      "source": [
        "# Standard List of Features\n",
        "\n",
        "Let's introduce the list of features we need to deploy a baseline learning-to-rank approach.\n",
        "\n",
        "We again cache the results of FeaturesRetriever to make it faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMG6P1cG5dRo"
      },
      "source": [
        "pagerankfile = indexref + \"/data-pagerank.oos\"\n",
        "\n",
        "# DO *NOT* CHANGE THIS LIST. Use PyTerrier operators to add features...\n",
        "features = [\n",
        "    \"SAMPLE\", #ie PL2 - this exposes the scores used to obtain the candidate set as a feature\n",
        "    \"WMODEL:SingleFieldModel(BM25,0)\", #BM25 title\n",
        "    \"QI:StaticFeature(OIS,%s)\" % pagerankfile,\n",
        "]\n",
        "\n",
        "stdfeatures = pt.terrier.FeaturesRetriever(index, features, verbose=True)\n",
        "if CACHE: # wrap in a cache transformer\n",
        "    stdfeatures = SparseScorerCache('features-cache', stdfeatures, value=\"features\", pickle=True, verbose=True)\n",
        "\n",
        "stage12 = firstpass >> stdfeatures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oFWy0u9143O"
      },
      "source": [
        "This is our feature set. We will be using FeaturesBatchRetrieve to compute these extra features on the fly. Let's see the output. You can see that there is now a \"features\" column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb-InOIq8Xc7"
      },
      "source": [
        "stage12.search(\"chemical reactions\").head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNoja4352Pvt"
      },
      "source": [
        "Let's look in more detail at the features. It is clear that there are 3 numbers for each document. The first is the PL2 score (1.27555456e+01 == 12.7555), the second is the BM25 score, and the third is the PageRank (a link analysis feature - discussed in more detail in Lecture 9)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6Horkb-2KYT"
      },
      "source": [
        "stage12.search(\"chemical reactions\").head(1).iloc[0][\"features\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYoJRrJR4oTL"
      },
      "source": [
        "# Q1\n",
        "\n",
        "You now have everyting you need to attempt Q1. You will need to refer to the specification, and to PyTerrier's [learning to rank documentation](https://pyterrier.readthedocs.io/en/latest/ltr.html).\n",
        "\n",
        "You should use a LightGBM LambdaMART implementation (*not* XGBoost), instantiated using the configuration suggested in the PyTerrier documentation.\n",
        "\n",
        "Hints:\n",
        " - You will need to use the provided separate “training” and “validation” topic sets and qrels to train the learning-to-rank.\n",
        " - There is no need to vary the configuration of LightGBM from that in the documentation.\n",
        " - Training and evaluating a LTR pipeline takes around 5 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNNPtj7XHsTs"
      },
      "source": [
        "#YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh0whrbS9-xu"
      },
      "source": [
        "# Q2 - URL Length Features\n",
        "\n",
        "In this block, please provide your code for Q2 concerning your two URL Length features, namely URL Length by counting slashes (URL-slashes) and URL Length through using the type of the URL (URL-type). The two different URL length features that you will need to implement are detailed in the specification. Do carefully read and follow the Exercise 2 specification before starting the implementation of the features.\n",
        "\n",
        "Some hints:\n",
        "\n",
        " - For computing each of your URL features, you will need to use an appropriate [pt.apply function](https://pyterrier.readthedocs.io/en/latest/apply.html). The dataframe of results obtained from the `firstpass` transformer has all of the information you need. You can see how fast your apply function is by setting `verbose=True`.\n",
        "\n",
        " - You can use the `**` PyTerrier operator for combining feature sets.\n",
        "\n",
        " - Refer to the PyTerrier learning to rank documentation  concerning `feature_importances_` for obtaining feature importances.\n",
        "\n",
        " - You may wish to refer to Python's [`urlparse()`](https://docs.python.org/3/library/urllib.parse.html) function.\n",
        "\n",
        " - Use Python assertions to test that your feature implmentation(s) give the expected results. **Remember that you need to report along your code all the tests you have conducted to ascertain the code's correctness.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeIqqLYtxB_h"
      },
      "source": [
        "## Q2 (a) URL-Slashes Feature\n",
        "\n",
        "In this block you should define your URL-Slashes feature, and **test it**. **Show clearly all the tests** that you have conducted to test that your feature works as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiNTl3HbxZwZ"
      },
      "source": [
        "#YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_ugoAqhxbGM"
      },
      "source": [
        "#### (i) URL-Slashes as a PL2 re-ranker\n",
        "\n",
        "Now you should evaluate your URL-slashes score by re-ranking PL2, without applying learning-to-rank.\n",
        "\n",
        "Hint:\n",
        " - Your reranker should order documents in descending order, i.e. longest URLs first.\n",
        "\n",
        " You can now answer the corresponding quiz questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVNHsX_ixoYS"
      },
      "source": [
        "#YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGoCpq3QxptA"
      },
      "source": [
        "#### (ii) URL-Slashes within an LTR model\n",
        "\n",
        "Now you should evaluate your URL-slashes score as a feature within a new learned model.\n",
        "\n",
        "Hint:\n",
        " - Carefully consider how to integrate your feature into an LTR model, based on your understanding of how a regression tree works.\n",
        "\n",
        "You can now answer the corresponding quiz questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leH5uh4xx2el"
      },
      "source": [
        "#YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo3mba6sxLqz"
      },
      "source": [
        "## Q2 (b) URL Type Feature\n",
        "\n",
        "In this block you should define your URL Type feature and **test it**. **Show clearly all the tests** you have conducted to test that your feature works as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgSh7TmmyIWN"
      },
      "source": [
        "#YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VugAPVF8yIWN"
      },
      "source": [
        "#### (i) URL Type as a PL2 re-ranker\n",
        "\n",
        "Now you should evaluate your URL type score by re-ranking PL2, without applying learning-to-rank.\n",
        "\n",
        "Hint:\n",
        " - Your reranker should order documents in descending order, i.e. longest URLs first.\n",
        "\n",
        "You can now answer the corresponding quiz questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8L4AVuVyIWO"
      },
      "source": [
        "#YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HuuiWUSyIWO"
      },
      "source": [
        "#### (ii) URL Type within an LTR model\n",
        "\n",
        "Now you should evaluate your URL type score as a feature within a new learned model.\n",
        "\n",
        "Hint:\n",
        " - Carefully consider how to integrate your feature into an LTR model, based on your understanding of how a regression tree works.\n",
        "\n",
        "You can now answer the corresponding quiz questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxBUGnDzyIWO"
      },
      "source": [
        "#YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7LtU56pC9VG"
      },
      "source": [
        "# Q3 Proximity Search Feature\n",
        "\n",
        "Now you will implement a new query-dependent feature, using the MinDist() function, as discussed in the specification. Do carefully **read the Exercise 2 specification** before starting the implementation.\n",
        "\n",
        "Hints:\n",
        " - Again, remember to use assertions to **test** your feature implementations.\n",
        " - Refer to the PyTerrier learning to rank documentation concerning `features_importances_` for obtaining feature importances\n",
        " - For tokenisation of queries and documents, you can simply use Python's [`str.split()`](https://docs.python.org/3.3/library/stdtypes.html#str.split), without any arguments. Do not use any external libraries.\n",
        "\n",
        "As mentioned in the specification, you should implement a function called `avgmindist()`, which takes the text of the query and the text of the document, and returns a score for the document, i.e. it must conform to the following Python specification:\n",
        "```python\n",
        "def avgmindist(query : str, document : str) -> float\n",
        "```\n",
        "\n",
        "**NB**: There are particular specific requirements for your implementations of MinDist() and avgmindist() that are detailed in the specification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiN0A-g7L0qz"
      },
      "source": [
        "#YOUR AVGMINDIST IMPLEMENTATION\n",
        "\n",
        "def avgmindist(query : str, document : str) -> float:\n",
        "  #update your implementation here.\n",
        "  return 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpCKKQBRL4o4"
      },
      "source": [
        "You should test your impementation yourself (your code must list along your code *all* the test cases you deployed to test that your feature works as expected). In addition, to also allow us to verify your implementation, we have created 9 testcases. Please run `run_test_cases()` and use its responses to answer the relevant quiz questions.\n",
        "\n",
        "Hint:\n",
        " - Our test cases took around 1-3ms each. If the testing of your implementation takes magnitudes of time longer, then this will impact upon how long it takes you to train and evaluate your implementation within a LTR pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DO NOT ALTER THIS CELL\n",
        "TEST_CASES = [\n",
        "  ('fermilab directory', 45, 567257), #1\n",
        "  ('webcam', 45, 567257), #2\n",
        "  ('DOM surface', 384034, 388292), #3\n",
        "  ('DOM surface', 45, 384034), #4\n",
        "  ('DOM surface document', 388292, 384034), #5\n",
        "  ('DOM software AMANDA', 639302, 384034), #6\n",
        "  ('fermilab directory', 388292, 384034), #7\n",
        "  ('trigger data', 596532, 639302), #8\n",
        "  ('underlying hardware', 384034, 333649) #9\n",
        "]\n",
        "\n",
        "def run_test_cases():\n",
        "  import datetime\n",
        "  docno=0\n",
        "  body=3\n",
        "  for i, (query, docid1, docid2) in enumerate(TEST_CASES):\n",
        "    start = datetime.datetime.now()\n",
        "    meta1 = index.getMetaIndex().getAllItems(docid1)\n",
        "    meta2 = index.getMetaIndex().getAllItems(docid2)\n",
        "    s1 = avgmindist(query, meta1[body])\n",
        "    s2 = avgmindist(query, meta2[body])\n",
        "    if s1 > s2:\n",
        "      result = meta1[docno]\n",
        "      cmpD = \"%s > %s\" % (meta1[docno],meta2[docno])\n",
        "    elif s2 > s1:\n",
        "      result = meta2[docno]\n",
        "      cmpD = \"%s > %s\" % (meta2[docno],meta1[docno])\n",
        "    else:\n",
        "      result = \"EQUAL\"\n",
        "      cmpD = \"%s == %s\" % (meta1[docno],meta2[docno])\n",
        "    end = datetime.datetime.now()\n",
        "    print(\"TEST CASE %d result %s time %d ms\" % (i+1, result, float((end-start).microseconds)/1000.))\n",
        "\n",
        "run_test_cases()"
      ],
      "metadata": {
        "id": "yWRFjHfsBToE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp57S1vTMiB5"
      },
      "source": [
        "You should now integrate your avgmindist() function into a new LTR model, and compare its MAP & P@5 performance to the LTR baseline. You can now answer the corresponding quiz questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaFXG5_PMnRN"
      },
      "source": [
        "#YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-akCsk7tDoOs"
      },
      "source": [
        "# Q4 A 5-feature Learning-to-Rank Model\n",
        "\n",
        "You will now experiment with the LightGBM LambdaMART technique where you include both your added features (URL Type and AvgMinDist) along with the 3 initial features inc the initial PL2 candidate set (5 features in total).\n",
        "\n",
        "You need to learn a *new* model when using your final selection of 5 features.\n",
        "\n",
        "Evaluate the performance of your resulting LTR system in comparison to the LTR baseline and answer the quiz questions. For ease of comparison and readability, you should also display your results for the performance of the 4-feature LTR models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25AQ4Mn2QAqg"
      },
      "source": [
        "#YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n_C4m5WEnMz"
      },
      "source": [
        "# That's all Folks\n",
        "\n",
        "**Submission Instructions:** Complete this notebook. All your answers to Exercise 2 must be submitted on the Exercise 2 Quiz instance on Moodle with your completed notebook (showing **both your solutions and the results of their executions**). Only answers submitted through the Quiz are marked though. Marks can be lost if the notebook does not **show evidence** for the reported answers in the quiz.\n",
        "\n",
        "While students are asked to submit their solutions through a Quiz, marking will be done with a “human-in-the-loop” and partial marks are awarded depending on the quality of the submitted work.\n",
        "\n",
        "Your answers to the Quiz questions along with your .ipynb notebook file (showing code and outputs) must be submitted by the stated Exercise 2 deadline."
      ]
    }
  ]
}