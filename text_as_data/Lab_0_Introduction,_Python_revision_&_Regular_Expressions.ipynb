{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayush-96/msc-data-science/blob/master/text_as_data/Lab_0_Introduction%2C_Python_revision_%26_Regular_Expressions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text as Data Lab 0: Introduction, Python revision & Regular Expressions\n",
        "\n",
        "The aims of the lab are to:\n",
        " - Introduce you to Colab, verify that you're set up with the correct python packages\n",
        " - Load textual data from Reddit as a JSON file\n",
        " - Explore the data to learn a bit about it\n",
        " - Review salient Python features, such as Counter and list comprehensions\n",
        " - Introduce regular expressions, a common tool for text matching and processing\n",
        "\n",
        "**Before you start, save a copy of this lab to your drive using \"File > Save a Copy in Drive\".** If you skip this step, you may lose progress that you have made (e.g., if you close the browser tab or your computer crashes)."
      ],
      "metadata": {
        "id": "ZlyN2JKWAxbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Colab Introduction\n",
        "\n",
        "\n",
        "Colab is a cloud-based Jupyter Notebook.  It is used internally by engineers and researchers at Google and companies worldwide to prototype and share data science and ML results in an easy-to-use way.\n",
        "\n",
        "It supports:\n",
        "\n",
        "1. Text Cells with [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) formatting\n",
        "2. Code Cells\n",
        "3. Notebook stores code, output, and execution order\n",
        "4. Tab and Tab + Tab Autocomplete\n",
        "5. IPython Help Features\n",
        "6. [IPython Magics (`%%`)](https://ipython.readthedocs.io/en/stable/interactive/magics.html) (such as [%%timeit](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit) to time the execution of a cell)\n",
        "\n",
        "### Additional Features\n",
        "\n",
        "- collaborative editing\n",
        "- history\n",
        "- comments\n",
        "- executed code history\n",
        "- Shift+click multiple cell selection\n",
        "- searchable code snippets + table of contents\n",
        "- scratchpad (⌘/Ctrl + Alt + N)\n",
        "\n",
        "### Keyboard Shortcuts\n",
        "| Command | Action |\n",
        "| ---- | ----: |\n",
        "|⌘/Ctrl+Enter | Run Selected Cell |\n",
        "|Shift+Enter| Run Cell and Select Next |\n",
        "|Alt+Enter| Run cell and insert new cell|\n",
        "|⌘/Ctrl+M I | Interrupt Execution |\n",
        "\n",
        "- You can open the command Palette to see all shortcuts by going to Tools --> Command palette.\n",
        "\n",
        "### Summary of tips\n",
        "- Use TAB to autocomplete an expression.\n",
        "- You can also execute the code with a ? to get the documentation (e.g. `?sum`)\n",
        "- In Jupyter / Colab you can execute shell commands using `!`, example: `!ls` to list the current files.\n",
        "\n",
        "*Note:* Occasionally Colab may hang or crash (due to cloud flakiness or bad code).  You can control the execution using the Runtime menu to reboot and start fresh.  To resume where you left off you can click \"Run before\" and it will run all cells before the one currently selected.\n"
      ],
      "metadata": {
        "id": "mAt5xuDsLBpN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading\n",
        "\n",
        "Let's start by downloading a file containing data scraped from the online forum platform [Reddit](https://www.reddit.com/). This should take at most a few seconds to run."
      ],
      "metadata": {
        "id": "yI5XsjPanCeL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoqbvVVMAvaX"
      },
      "outputs": [],
      "source": [
        "# Download data\n",
        "!wget -O reddit_posts.json https://gla-my.sharepoint.com/:u:/g/personal/jake_lever_glasgow_ac_uk/EY_R8Y7DkrxMqXGe-zlgeNkBdJU5ZNTf8FYrN2pqDwddMA?download=1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration\n",
        "\n",
        "If we take a look at the contents of the data file, we see that it is encoded as an array of objects in [JSON](https://en.wikipedia.org/wiki/JSON). There are a variety of other formats used for sharing text data (e.g., [XML](https://en.wikipedia.org/wiki/XML), [Protocol Buffers](https://en.wikipedia.org/wiki/Protocol_Buffers), etc.), but JSON is among the most common these days."
      ],
      "metadata": {
        "id": "A2NGM0ZDnp65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the first 20 lines of the file - note the exclamation mark which tells Colab to run a terminal command instead of Python\n",
        "!head -n 20 reddit_posts.json"
      ],
      "metadata": {
        "id": "d7homfd9ndq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data provided by online APIs, such as the [Reddit API](https://www.reddit.com/dev/api/), are usually available in JSON. For this lab, we use a simplified version of the Reddit data, aggregating posts across several subreddits (i.e., sub-forums) and using just a handful of salient fields for each post. The data file consists of some metadata about the posts (`subreddit`, `score`, `id`, and `author`) and two fields with the text of the posts (`title` and `body`)."
      ],
      "metadata": {
        "id": "BuBN2-KoGuep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's load the data into Python so we can work with it. The [json](https://docs.python.org/3/library/json.html) package in the Python standard library makes loading the data very easy."
      ],
      "metadata": {
        "id": "RcydjzEdGx_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data into Python\n",
        "import json\n",
        "with open('reddit_posts.json', 'rt') as fin:\n",
        "  reddit_posts = json.load(fin)"
      ],
      "metadata": {
        "id": "iRz19a-7A4iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python's JSON parser provides the data as a `list` of `dict` objects. You should already be familiar with these classes; if not, you can refer to [Python's data structures documentation](https://docs.python.org/3/tutorial/datastructures.html).\n",
        "\n",
        "Let's look at the types of data available provided and some basic information."
      ],
      "metadata": {
        "id": "GKe3TU9HHnrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Investigate the structure of the data\n",
        "print('type(reddit_posts) =', type(reddit_posts))\n",
        "print('len(reddit_posts) =', len(reddit_posts))\n",
        "print('type(reddit_posts[0]) =', type(reddit_posts[0]))\n",
        "print('reddit_posts[0].keys() =', reddit_posts[0].keys())\n",
        "print('reddit_posts[0][\"title\"] =', reddit_posts[0][\"title\"])"
      ],
      "metadata": {
        "id": "xinvnfVGBKoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that there are 2000 posts in the dataset. It's always important to understand the data you're working with, so let's see which subreddits these posts are from.\n",
        "\n",
        "We could write code to loop through each of the posts and keep a running count of the number of posts in each subreddit:\n",
        "\n",
        "```python\n",
        "# The code that uses Counter below essentially does this:\n",
        "subreddit_counts = {}\n",
        "for post in reddit_posts:\n",
        "  subreddit = post['subreddit']\n",
        "  if subreddit not in subreddit_counts:\n",
        "    subreddit_counts[subreddit] = 0\n",
        "  subreddit_counts[subreddit] += 1\n",
        "subreddit_counts\n",
        "```\n",
        "\n",
        "Python provides a convenient [`Counter`](https://docs.python.org/3/library/collections.html#collections.Counter) class to count values like this, so we'll use that instead. We can use a [Generator Expression](https://docs.python.org/3/reference/expressions.html#generator-expressions) to get the subreddit from each post and let `Counter` count how many times each subreddit appears."
      ],
      "metadata": {
        "id": "ku6Iri8SKikQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of times each subreddit appears in this dataset\n",
        "from collections import Counter\n",
        "Counter(post['subreddit'] for post in reddit_posts)"
      ],
      "metadata": {
        "id": "o8_WlxMrKNWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that there are 9 subreddits covered by the dataset, mostly about beverages and gaming. The Soda subreddit is the least common (174 posts) and the NintendoSwitch subreddit is the most common (249 posts).\n",
        "\n",
        "**Exercise:** Can you find the user with the most posts and how many posts they have? (Hint: `Counter` provides a function that will help you find the item with the highest count, without needing to look through all the counts manually. You can find this in the [documentation](https://docs.python.org/3/library/collections.html#collections.Counter).)"
      ],
      "metadata": {
        "id": "PZmd8evUNxmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your answer here"
      ],
      "metadata": {
        "id": "b9imZr5MK3nT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should find that AutoModerator has the most number of posts, with 5.\n",
        "\n",
        "AutoModerator sounds like it may not be a human user, so let's take a look at their posts. There are several ways to do this filtering. One of the most concise is to use a [List Comprehension](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions), so we'll do that here:"
      ],
      "metadata": {
        "id": "nujmWnKnPHUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[ post for post in reddit_posts if post['author'] == 'AutoModerator' ]"
      ],
      "metadata": {
        "id": "83yPQ-tPPSms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed, through qualitatively inspecting of AutoModerator's posts, they appear to be automatically generated.\n",
        "\n",
        "Depending on your application, you may want to filter out such posts. For instance, if you were trying to measure public sentiment about a product, you are probably only interested in human users. You could filter out auto-generated posts by manually checking each post, but this becomes impractical as the size of your collection grows. Later in this course we will cover text classification techniques, which could be used to automatically label a large number of posts given manually-labeled training data.\n",
        "\n"
      ],
      "metadata": {
        "id": "iS6VaixxH2MZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you find out anything else interesting from the dataset by inspecting the data?"
      ],
      "metadata": {
        "id": "d2m3Fua5J9aA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this cell to explore the dataset. (Feel free to make other cells as well.)"
      ],
      "metadata": {
        "id": "czSDK1yUKFst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regular Expressions\n",
        "\n",
        "[Regular Expressions](https://en.wikipedia.org/wiki/Regular_expression) (often abbreviated as regex or regexp) are a common tool for pattern matching in text. Python provides regular expressions in the [`re`](https://docs.python.org/3/library/re.html) package.\n",
        "\n",
        "Regular expressions can get very complicated, but it is valuable to be familiar with them. This portion of the lab provides a basic introduction to regex and provides links to further details if you want to learn more."
      ],
      "metadata": {
        "id": "6ZZNGQhoVDqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Motivating Example\n",
        "\n",
        "Let's say you want to find all posts that mention [Irn-Bru](https://en.wikipedia.org/wiki/Irn-Bru). One option would be to use the string contains operator (`in`):"
      ],
      "metadata": {
        "id": "W7NC0yTaWKvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[ p for p in reddit_posts if 'Irn-Bru' in p['title'] or 'Irn-Bru' in p['body'] ]"
      ],
      "metadata": {
        "id": "IB5M4O4ITQ6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hmm, nothing matches? Didn't we see the first post mentioned Irn-Bru?"
      ],
      "metadata": {
        "id": "qFHHlZktYNON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_posts[0]"
      ],
      "metadata": {
        "id": "uqtxBz8PYMmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed so, but they stylized it as \"Irn Bru\" rather than \"Irn-Bru\". There's probably other ways people might write it too, like IrnBru, Irnbru, etc. We could come up with a list of possibilities and control for different casing (like the code below), but we still might miss some. There's a simpler way using regular expressions.\n",
        "\n",
        "```python\n",
        "match_strings = ['irn-bru', 'irn bru', 'irnbru']\n",
        "[ p for p in reddit_posts if any(m in p['title'].lower() or m in p['body'].lower() for m in match_strings) ]\n",
        "```\n",
        "\n",
        "### Filtering with Regular Expressions\n",
        "\n",
        "A regular expression defines a pattern to match in text as a string. Most characters (letters, numbers) in a pattern match themselves. Some perform special functions, like making the previous character optional or allowing multiple characters to match. For instances:\n",
        " - \"`.`\" matches any character.\n",
        " - \"`?`\" makes the previous character optional.\n",
        "\n",
        "With this, we can define the regular expression `\"irn.?bru\"`, which will match `irn` and `bru` with any character (or no character) between them. An option allows case insensitive matching.\n",
        "\n",
        "We can use regular expressions in python by first importing the `re` package and then compiling our pattern."
      ],
      "metadata": {
        "id": "7WTMGfb8Yazn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = re.compile(r'irn.?bru', re.I) # re.I makes the pattern case insensitive"
      ],
      "metadata": {
        "id": "zdwtfULaY0ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the string with the regular expression has an `r` before. What's that about? That denotes that it is a raw string literal. It means that any [escape sequences](https://docs.python.org/3/reference/lexical_analysis.html#escape-sequences) (e.g. `\\t` for a tab or `\\\\` for a backslash) are ignored. It's often a good idea to use raw string literals (i.e. putting an `r` before the string) for regular expressions. Regular expressions use a lot of backslashes, and it's annoying to have to escape them repeatedly (i.e. having to write `\\\\` instead of `\\` every time).\n",
        "\n",
        "The pattern object can do all sorts of things. Most commonly, you will use the `search` function, which finds and returns the first place that the pattern matches a string."
      ],
      "metadata": {
        "id": "HdPqbd3niIzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "match = pattern.search(\"Anyone tried Irn Bru?\")\n",
        "match"
      ],
      "metadata": {
        "id": "IAnKSfq5iN4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The match object gives the character offsets of the match (`match.span`) and the text itself that matches (`match.group(0)`).\n",
        "\n",
        "If no match is found, `search` returns `None`, which evaluates as `False` when it's used in an `if` statement. This allows it to be easily used for filtering data. Using regular expressions, we find 6 posts about Irn-Bru."
      ],
      "metadata": {
        "id": "tYbnVJPXiglC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[ p for p in reddit_posts if pattern.search(p['title']) or pattern.search(p['body']) ]"
      ],
      "metadata": {
        "id": "PlxoWsLShHwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there are a variety of ways that people express Irn-Bru.\n",
        "\n",
        "**Exercise:** Can you write code that finds all the ways people express Irn-Bru in the dataset (e.g., \"Irn-Bru\", \"IRN BRU\", etc.)? Hint: you probably need to use the [`findall` function](https://docs.python.org/3/library/re.html#re.findall) with the `pattern` already defined above."
      ],
      "metadata": {
        "id": "QNcswjeikHz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your answer here"
      ],
      "metadata": {
        "id": "J8-GabEslnMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(You should find that \"Irn Bru\" appears 6 times, \"IRN-BRU\" appears twice, \"IRN BRU\" once, and \"irn bru\" once.)"
      ],
      "metadata": {
        "id": "6Bq4NihLnRqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try to find some years mentioned in the text of the documents. you'll need to define a new pattern. You might find that a [regular expressions cheatsheet](https://cheatography.com/davechild/cheat-sheets/regular-expressions/) is helpful so you can look up the pattern for `digit`.\n",
        "\n",
        "**Exercise:** Find all 21st-century years (4 digit numbers starting with '20') in the title and bodies of the posts. What is the most common?"
      ],
      "metadata": {
        "id": "fUClndGDdg0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your answer here"
      ],
      "metadata": {
        "id": "NCJ8HvwMdhrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[ p['title'] for p in reddit_posts if re.search(r'^(.).*\\1$', p['title']) ]"
      ],
      "metadata": {
        "id": "b5QaCDanvV-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should find that the digits `2016` appears 35 times across the posts.\n",
        "\n",
        "Now, a question: do you think all of those 4 digit numbers are actually years in the text. Could they be anything else?"
      ],
      "metadata": {
        "id": "93uo_7WHeZX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additional Resources\n",
        "\n",
        "These examples only scratch the surface of the capacities of regular expressions. We refer you to these resources for further details.\n",
        "\n",
        " - [RegExr](https://regexr.com/) -- Interactively build and evaluate regular expressions in a GUI. This can be helpful when trying to build a tricky regex or figure out why it's not matching what you want it to.\n",
        " - [RegexOne](https://regexone.com/) -- A detailed and interactive regular expression tutorial\n",
        " - [Python's `re` documentation](https://docs.python.org/3/library/re.html) -- Provides details about both regex syntax and Python's regex API"
      ],
      "metadata": {
        "id": "a5e4crdKVFSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End\n",
        "\n",
        "This is the end of Lab 0. Let us know if you encounter any issues! Remember we are here to help!\n",
        "\n",
        "In this lab, you...\n",
        "- started using Colab\n",
        "- loaded textual data from Reddit as a JSON file\n",
        "- explored the data to learn a bit about it\n",
        "- reviewed salient Python features, such as Counter and list comprehensions\n",
        "- explored using regular expressions in Python\n",
        "\n",
        "**Please remember to submit your completed lab and feedback form on Moodle.**\n"
      ],
      "metadata": {
        "id": "hIeO1UeINAjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional Extras\n",
        "\n",
        "The [cheatsheet](https://cheatography.com/davechild/cheat-sheets/regular-expressions/) may be handy for some of these. See if you can come up with a single regular expression for each problem.\n",
        "\n",
        "- Count how many Reddit posts in our dataset have titles that start with a digit followed by any character that isn't a digit (e.g. \"1A\" or \"7-\").\n",
        "- Count how many Reddit posts in our dataset have two 4-digit numbers in the body of the post? What about 4-digit numbers that are not part of larger bits of text (e.g. are properly separated - you could use `\\b` to indicate a [word boundary](https://www.regular-expressions.info/wordboundaries.html)).\n",
        "- Find Reddit posts where the title starts and ends with the same character. You likely need to use [backreferences](https://www.regular-expressions.info/backref.html) in the regular expression.\n",
        "- Write a regex that matches to [British postcodes](https://ideal-postcodes.co.uk/guides/uk-postcode-format) (e.g. G12 8RZ) and is agnostic if there is a space in the middle. You can make it fairly general or try to be more specific to the intricacies of the postcode standards. See if there are any matches in the Reddit posts. Are they actually postcodes?\n",
        "- Write a regex using the [re.sub function](https://docs.python.org/3/library/re.html#re.sub) that swaps the first part (the outcode) and second part (the incode) of a postcode (G128RZ -> 8RZG12). You could use backreferences as mentioned in the [re.sub documentation](https://docs.python.org/3/library/re.html#re.sub)."
      ],
      "metadata": {
        "id": "lQa3CLJveFMW"
      }
    }
  ]
}